{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c5e8de",
   "metadata": {},
   "source": [
    "# Importing libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f37ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28702176",
   "metadata": {},
   "source": [
    "# Updating Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"merged_data.csv\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Assuming you have a DataFrame called df\n",
    "df = df[::-1]\n",
    "\n",
    "# Assuming you have a DataFrame called df with a 'Date' column\n",
    "df['Year'] = pd.to_datetime(df['Date']).dt.year\n",
    "\n",
    "\n",
    "df = df.drop('Date', axis=1)\n",
    "\n",
    "# Calculate the percentage change in stock price\n",
    "df['Percentage Change'] = df['Close'].pct_change()\n",
    "\n",
    "# Calculate RSI\n",
    "df['rsi'] = ta.momentum.rsi(df['Close'])\n",
    "\n",
    "# Calculate Stochastic RSI\n",
    "df['stoch_rsi'] = ta.momentum.stochrsi(df['Close'])\n",
    "\n",
    "# Calculate Fibonacci levels\n",
    "fibonacci_levels = []\n",
    "prev_level = df['Close'].min()  # Starting from the minimum value of Close\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i % 169 == 0:  # Calculate Fibonacci level every 169 rows (adjust as needed)\n",
    "        level = prev_level + (df['Close'].max() - prev_level) * 0.618\n",
    "        fibonacci_levels.append(level)\n",
    "        prev_level = level\n",
    "    else:\n",
    "        fibonacci_levels.append(np.nan)\n",
    "\n",
    "df['fibonacci'] = np.array(fibonacci_levels)\n",
    "\n",
    "# Calculate MACD\n",
    "df['macd'] = ta.trend.macd(df['Close'])\n",
    "\n",
    "# Define the EPS, DPS, and P/E values corresponding to each year\n",
    "eps_values = {\n",
    "    2013: 3.50,\n",
    "    2014: 4.30,\n",
    "    2015: 3.70,\n",
    "    2016: 4.10,\n",
    "    2017: 4.50,\n",
    "    2018: 5.30,\n",
    "    2019: 5.70,\n",
    "    2020: 6.10,\n",
    "    2021: 7.30,\n",
    "    2022: 8.50,\n",
    "    2023: 31.65\n",
    "}\n",
    "\n",
    "dps_values = {\n",
    "    2013: 1.80,\n",
    "    2014: 2.20,\n",
    "    2015: 1.90,\n",
    "    2016: 2.10,\n",
    "    2017: 2.30,\n",
    "    2018: 2.70,\n",
    "    2019: 2.90,\n",
    "    2020: 3.10,\n",
    "    2021: 3.70,\n",
    "    2022: 4.30,\n",
    "    2023: 4.50\n",
    "}\n",
    "\n",
    "pe_values = {\n",
    "    2013: 13.00,\n",
    "    2014: 11.00,\n",
    "    2015: 12.70,\n",
    "    2016: 10.70,\n",
    "    2017: 9.78,\n",
    "    2018: 8.25,\n",
    "    2019: 7.89,\n",
    "    2020: 7.05,\n",
    "    2021: 5.96,\n",
    "    2022: 5.05,\n",
    "    2023: 63.19\n",
    "}\n",
    "\n",
    "# Match the years and update the EPS, DPS, and P/E columns\n",
    "df['EPS (NRs)'] = df['Year'].map(eps_values)\n",
    "df['DPS (NRs)'] = df['Year'].map(dps_values)\n",
    "df['P/E'] = df['Year'].map(pe_values)\n",
    "\n",
    "# Print the updated DataFrame with the calculated indicators\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ce578",
   "metadata": {},
   "source": [
    "# Standerdizing the Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "df_standardized = df.copy()\n",
    "\n",
    "# Select the columns to standardize\n",
    "columns_to_standardize = ['Open', 'High', 'Low','Volume', 'Sentiment', 'bonus', 'dividend', 'Percentage Change','5-day SMA','8-day SMA','13-day SMA','50-day MA','200-day MA','stoch_rsi','rsi','macd','EPS (NRs)','DPS (NRs)','P/E']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the selected columns\n",
    "df_standardized[columns_to_standardize] = scaler.fit_transform(df_standardized[columns_to_standardize])\n",
    "\n",
    "# Print the standardized DataFrame\n",
    "print(df_standardized)\n",
    " \n",
    "df=df_standardized\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738363f9",
   "metadata": {},
   "source": [
    "# Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ccb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#data into features and target\n",
    "X = df.drop( ['sn','Symbol','8-day SMA','13-day SMA','50-day MA','200-day MA','stoch_rsi','rsi','macd','EPS (NRs)','DPS (NRs)','P/E'], axis=1)\n",
    "y = df['Close']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=186)\n",
    "\n",
    "# Train the model\n",
    "reg_RF = RandomForestRegressor(random_state=186)\n",
    "reg_RF.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_RF = reg_RF.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('The score for RandomForest is {}'.format(reg_RF.score(X_test, y_test)))\n",
    "\n",
    "# Visualize feature importances\n",
    "d = pd.DataFrame(index=X.columns, data=reg_RF.feature_importances_, columns=['Importance']).sort_values('Importance')\n",
    "d.plot(kind='barh', figsize=(10,5))\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec99898",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5313ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Layer, Bidirectional, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the attention layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['sn','Symbol'], axis=1).values\n",
    "y = df['Close'].values\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "batch_sizes = [32, 64, 128]\n",
    "random_states = [164, 328, 492]\n",
    "\n",
    "best_r2 = -np.inf\n",
    "best_dropout_rate = None\n",
    "best_batch_size = None\n",
    "best_random_state = None\n",
    "\n",
    "# Perform grid search\n",
    "for dropout_rate in dropout_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for random_state in random_states:\n",
    "            print(f\"Testing dropout_rate={dropout_rate}, batch_size={batch_size}, random_state={random_state}\")\n",
    "\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "            # Reshape the input data\n",
    "            time_steps = 23 # Number of time steps (previous days' data)\n",
    "            features = 1 # Number of features\n",
    "            X_train_reshaped = X_train.reshape(X_train.shape[0], time_steps, features)\n",
    "            X_test_reshaped = X_test.reshape(X_test.shape[0], time_steps, features)\n",
    "\n",
    "            # Create the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(time_steps, features)))\n",
    "            model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Attention())\n",
    "            model.add(Dense(units=1, activation='linear')) # Linear activation for regression\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train_reshaped, y_train, epochs=3000, batch_size=batch_size, verbose=0)\n",
    "\n",
    "            # Evaluate the model\n",
    "            y_pred = model.predict(X_test_reshaped)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            print(\"R-squared:\", r2)\n",
    "\n",
    "            # Update the best parameters if the current model is better\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_dropout_rate = dropout_rate\n",
    "                best_batch_size = batch_size\n",
    "                best_random_state = random_state\n",
    "\n",
    "# Print the best parameters and corresponding R-squared value\n",
    "print(\"Best Dropout Rate:\", best_dropout_rate)\n",
    "print(\"Best Batch Size:\", best_batch_size)\n",
    "print(\"Best Random State:\", best_random_state)\n",
    "print(\"Best R-squared:\", best_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04f714",
   "metadata": {},
   "source": [
    "# CNN for stock market prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b52918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense,Flatten, Layer, Bidirectional, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['sn', 'Symbol','Close','Open','High','Low'], axis=1).values\n",
    "y = df['Close'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=164)\n",
    "\n",
    "# Reshape the input data\n",
    "time_steps = 18 # Number of time steps (previous days' data)\n",
    "features = 1 # Number of features\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], time_steps, features)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], time_steps, features)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1, activation='linear')) # Linear activation for regression\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=3300, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f0acf",
   "metadata": {},
   "source": [
    "# Convolutional neural network- Attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8977cfb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3436\\1907016572.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Split the data into features (X) and target variable (y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Symbol'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'High'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Layer, Bidirectional, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the attention layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['sn', 'Symbol','Close','Open','High','Low'], axis=1).values\n",
    "y = df['Close'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=164)\n",
    "\n",
    "# Reshape the input data\n",
    "time_steps = 18 # Number of time steps (previous days' data)\n",
    "features = 1 # Number of features\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], time_steps, features)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], time_steps, features)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Attention())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1, activation='linear')) # Linear activation for regression\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=3300, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d3fbc",
   "metadata": {},
   "source": [
    "# CNN-biDirectional LSTM-AM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense,Flatten, Layer, Bidirectional, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the attention layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['sn', 'Symbol','Close','Open','High','Low'], axis=1).values\n",
    "y = df['Close'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=164)\n",
    "\n",
    "# Reshape the input data\n",
    "time_steps = 18 # Number of time steps (previous days' data)\n",
    "features = 1 # Number of features\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], time_steps, features)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], time_steps, features)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Dropout(0.2))  # Dropout layer with a rate of 0.2\n",
    "model.add(Attention())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1, activation='linear')) # Linear activation for regression\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=3300, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
